services:
  ollama-intel-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        IPEXLLM_RELEASE_REPO: ajaymnair/ollama-intel-gpu
        IPEXLLM_RELEASE_VERSON: v2.3.0-nightly
        IPEXLLM_PORTABLE_ZIP_FILENAME: ollama-ipex-llm-2.3.0b20250612-ubuntu.tgz
    container_name: ollama-intel-gpu
    restart: always
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - ollama-intel-gpu:/root/.ollama
    environment:
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
      - IPEX_LLM_NUM_CTX=16384
volumes:
  ollama-intel-gpu: {}
